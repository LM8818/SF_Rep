# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Banking NLP System
"""
import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
import pandas as pd
from ..utils.data_generator import BankingDataGenerator

import logging
logger = logging.getLogger(__name__)

class DataInitializer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.processed_dir = self.data_dir / "processed"
        self.raw_dir = self.data_dir / "raw"
        self.conversations_file = self.processed_dir / "conversations_processed.csv"
        self.metadata_file = self.processed_dir / "data_metadata.json"
        
    async def ensure_data_available(self) -> bool:
        """–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –∏—Ö –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö
        if self.conversations_file.exists():
            logger.info("üìä –ù–∞–π–¥–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ: {self.conversations_file}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
            if await self._is_data_fresh():
                logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
                return True
            else:
                logger.info("üîÑ –î–∞–Ω–Ω—ã–µ —É—Å—Ç–∞—Ä–µ–ª–∏, –æ–±–Ω–æ–≤–ª—è–µ–º...")
        else:
            logger.info("üìÇ CSV –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∑–∞–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        await self._generate_fresh_data()
        return True
    
    async def _is_data_fresh(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not self.metadata_file.exists():
            return False
            
        try:
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã –Ω–µ –±–æ–ª–µ–µ 7 –¥–Ω–µ–π –Ω–∞–∑–∞–¥
            creation_date = datetime.fromisoformat(metadata.get('creation_date', ''))
            days_old = (datetime.now() - creation_date).days
            
            return days_old < 7 and metadata.get('conversations_count', 0) >= 500
            
        except (json.JSONDecodeError, ValueError, KeyError):
            return False
    
    async def _generate_fresh_data(self) -> None:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ"""
        logger.info("üîß –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π...")
        self._create_directories()
        
        logger.info("ü§ñ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤...")
        generator = BankingDataGenerator()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ –¥–ª—è –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∏—è
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            None, 
            generator.generate_csv_files, 
            1000  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        await self._save_metadata(1000)
        
        logger.info("‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    
    def _create_directories(self) -> None:
        """–°–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        for directory in [self.data_dir, self.processed_dir, self.raw_dir]:
            directory.mkdir(parents=True, exist_ok=True)
    
    async def _save_metadata(self, conversations_count: int) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        metadata = {
            "creation_date": datetime.now().isoformat(),
            "conversations_count": conversations_count,
            "data_version": "1.0",
            "generator_version": "auto",
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    async def get_data_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not self.conversations_file.exists():
            return {"status": "no_data", "total_conversations": 0}
        
        try:
            # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Å—Ç—Ä–æ–∫
            df = pd.read_csv(self.conversations_file)
            
            metadata = {}
            if self.metadata_file.exists():
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            
            return {
                "status": "available",
                "total_conversations": len(df),
                "themes_available": df['theme'].nunique() if 'theme' in df.columns else 0,
                "products_available": df['product'].nunique() if 'product' in df.columns else 0,
                "last_updated": metadata.get("last_updated", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
                "file_path": str(self.conversations_file)
            }
            
        except Exception as e:
            return {"status": "error", "error": str(e), "total_conversations": 0}
    
    async def get_conversations_data(self, limit: Optional[int] = None) -> pd.DataFrame:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤"""
        if not self.conversations_file.exists():
            await self.ensure_data_available()
        
        df = pd.read_csv(self.conversations_file)
        
        if limit:
            return df.head(limit)
        
        return df
